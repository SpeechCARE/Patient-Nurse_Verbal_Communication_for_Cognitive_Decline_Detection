{"cells":[{"cell_type":"code","execution_count":null,"id":"3a950e77","metadata":{"id":"3a950e77"},"outputs":[],"source":["import ast\n","import pandas as pd\n","import numpy as np\n","import glob\n","import random\n","import os\n","import re\n","from datetime import datetime\n","import pyreadr\n","import itertools\n","from functools import reduce\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_extraction.text import CountVectorizer #convert text comment into a numeric vector\n","from sklearn.feature_extraction.text import TfidfTransformer #use TF IDF transformer to change text vector created by count vectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC# Support Vector Machine\n","from sklearn.pipeline import Pipeline #pipeline to implement steps in series\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.linear_model import ElasticNetCV\n","from sklearn.linear_model import LogisticRegression\n","from imblearn.over_sampling import SMOTE\n","import warnings\n","warnings.filterwarnings('ignore')\n","import preprocessor as p\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score, roc_curve, auc\n","from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, KFold, LeaveOneOut\n","from IPython.display import clear_output\n","import gensim\n","from sklearn.model_selection import train_test_split\n","import textblob\n","nltk.download('averaged_perceptron_tagger')\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","import mifs\n","from sklearn.utils.class_weight import compute_sample_weight\n","from collections import Counter\n","pd.set_option('display.max_colwidth', None)\n","import torch\n","import transformers\n","from  transformers import DistilBertModel, DistilBertTokenizer, AutoModel, AutoTokenizer\n","\n","#######################################################################\n","#word2vec\n","from gensim.models import KeyedVectors\n","from gensim.models import Word2Vec\n","import logging\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n"]},{"cell_type":"code","execution_count":null,"id":"a5107665","metadata":{"id":"a5107665"},"outputs":[],"source":["path = \"/Users/Maryam/Library/Mobile Documents/com~apple~CloudDocs/_____________projects/ED_visit_speech_clinicalNotes_OASIS/DATA/\"\n","\n","path_data = \"/Users/Maryam/Library/Mobile Documents/com~apple~CloudDocs/_____________projects/___predicting_cogntive_status_symptoms/DATA/\"\n","\n","path_results = \"/Users/Maryam/Library/Mobile Documents/com~apple~CloudDocs/_____________projects/___predicting_cogntive_status_symptoms/RESULTS/\""]},{"cell_type":"code","execution_count":null,"id":"781e8e18","metadata":{"id":"781e8e18"},"outputs":[],"source":["# apply threshold to positive probabilities to create labels\n","def to_labels(pos_probs, threshold):\n","    return (pos_probs >= threshold).astype('int')\n","\n","def classifier_results(X, y, t, clf):\n","    loo = LeaveOneOut()\n","    all_y_pred_probs = []\n","    y_tests = []\n","    for i, (train_idx, test_idx) in enumerate(loo.split(X)):\n","        X_train, y_train = X[train_idx], y[train_idx]\n","        X_test, y_test = X[test_idx], y[test_idx]\n","        X_train_t = t.fit_transform(X_train)\n","        X_test_t = t.transform(X_test)\n","        model = clf.fit(X_train_t, y_train)\n","        y_pred_probs = model.predict_proba(X_test_t)\n","        all_y_pred_probs.append(y_pred_probs.ravel().tolist()+y_test.tolist())\n","        y_tests.append(y_test[0])\n","    all_y_pred_probs = np.array(all_y_pred_probs)\n","    # Optimum F1 score\n","    ## keep probabilities for the positive outcome only\n","    probs = all_y_pred_probs[:, 1]\n","    ## define thresholds\n","    thresholds = np.arange(0, 1, 0.001)\n","    ## evaluate each threshold\n","    f1_scores = [f1_score(y_tests, to_labels(probs, t)) for t in thresholds]\n","    ## get best threshold\n","    ix = np.argmax(f1_scores)\n","    f1_score_optimum = round(f1_scores[ix]*100, 2)\n","    ## get f2_score, precision, and recall based on f1_score_optimum\n","    f2_score = round(fbeta_score(y_tests, to_labels(probs, thresholds[ix]), beta=2)*100, 2)\n","    precision = round(precision_score(y_tests, to_labels(probs, thresholds[ix]))*100, 2)\n","    recall = round(recall_score(y_tests, to_labels(probs, thresholds[ix]))*100, 2)\n","    # AUC\n","    fpr, tpr, threshs = roc_curve(y_tests, probs)\n","    auc_score = auc(fpr, tpr)\n","    auc_score = round(auc_score*100, 2)\n","    return f1_score_optimum, auc_score, f2_score, precision, recall, round(thresholds[ix]*100, 2), all_y_pred_probs\n","\n","\n","seed=55\n","clf_svm_rbf = SVC(kernel='rbf', probability=True, random_state=seed)#, class_weight='balanced')\n","clf_svm_lin = SVC(kernel='linear', probability=True, random_state=seed)\n","clf_xgb = XGBClassifier(random_state=seed)\n","clf_lr = LogisticRegression(random_state=seed)\n","clfs_list = [clf_svm_rbf, clf_svm_lin, clf_xgb, clf_lr]\n","clfs_name = ['SVM-rbf', 'SVM-linear', 'XGB', 'LR']\n","\n","df_result = pd.DataFrame(columns=['Approach', 'Pipline', 'pretrain_weights', 'embedding_structure',\n","                                  'Using_jmim', 'Classifier',\n","                                  'Best_threshold_MinMax', 'F1-score_MinMax', 'AUC_MinMax','F2-score_MinMax',\n","                                  'Precision_MinMax', 'Recall_MinMax',\n","                                  'Best_threshold_StandardScaler', 'F1-score_StandardScaler', 'AUC_StandardScaler',\n","                                  'F2-score_StandardScaler', 'Precision_StandardScaler', 'Recall_StandardScaler'\n","                                  ])"]},{"cell_type":"markdown","id":"b68e1d90","metadata":{"id":"b68e1d90"},"source":["# Classification"]},{"cell_type":"code","execution_count":null,"id":"80c7a4da","metadata":{"id":"80c7a4da"},"outputs":[],"source":["def classifying_all(df, clfs_list, clfs_name, get_probability=False, get_minmax=False, approach=None,\n","                    pipline=None, pretrain_weights=None, embedding_structure=None, using_jmim=False,\n","                    stan_scal=False, unit_vector=False, nurse=False, most_important_features=False,\n","                    n_features_jmim='auto'):\n","\n","    if 'date_time' in df.columns.tolist():\n","        df = df.drop(['date_time'], axis=1)\n","\n","    if nurse:\n","        add_to_columns = '_ns'\n","    else:\n","        add_to_columns = '_pt'\n","    # Tf-IDF\n","    if pipline=='TF-IDF' or pipline=='UMLS':\n","        if pipline=='UMLS':\n","            variable = 'cui_result_to_tfidf'\n","        else:\n","            variable = 'text'\n","\n","        tfidf = TfidfVectorizer(stop_words= {'english'})\n","        X_tf = tfidf.fit_transform(df[variable].to_numpy()).toarray()\n","        df_new = pd.DataFrame(data=X_tf, columns=np.array(tfidf.get_feature_names(), dtype=object)+add_to_columns).copy()\n","    # LIWC\n","    elif pipline=='LIWC':\n","        df_new = df.drop(['study_id', 'text', 'outcome', 'hosp_ed_ind'], axis=1).copy()\n","        df_new.columns = df_new.columns + add_to_columns\n","    # OASIS\n","    elif pipline=='OASIS':\n","        df_new = df.drop(['study_id', 'oasis_id', 'outcome'], axis=1).copy()\n","    # Turn taking feature\n","    elif pipline=='Turn_taking':\n","        df_new = df.drop(['study_id', 'outcome', 'hosp_ed_ind'], axis=1).copy()\n","    # Other pipline\n","    else:\n","        df_new = df.drop(['study_id', 'outcome'], axis=1).copy()\n","\n","    X = df_new.to_numpy()\n","    y = df['outcome'].to_numpy()\n","\n","    if using_jmim:\n","        t = MinMaxScaler()\n","        if stan_scal:\n","            t = StandardScaler()\n","        X_t = t.fit_transform(X)\n","\n","        if unit_vector:\n","            ratio = np.sqrt(np.sum(np.square(X_t), axis=1))\n","            ratio = ratio.reshape(ratio.shape[0], 1)\n","            X_t = X_t/ratio\n","\n","        MIFS = mifs.MutualInformationFeatureSelector(method='JMIM', k=5, n_features=n_features_jmim, verbose=2)\n","        MIFS.fit_transform(X_t, df['outcome'])\n","\n","        df_new_jm = df_new.iloc[:, MIFS.ranking_]\n","        X = df_new_jm.to_numpy()\n","        df_new_jm['study_id'] = df['study_id']\n","        df_new_jm['outcome'] = df['outcome']\n","\n","        if using_jmim and most_important_features:\n","            clear_output()\n","            df_mif = pd.DataFrame({'Selected feature':df_new.columns[MIFS.ranking_], 'JMIM value':MIFS.mi_})\n","            return df_mif.sort_values('JMIM value', ascending=False)\n","\n","    row = 0\n","    for clf, clf_name in zip(clfs_list, clfs_name):\n","        f1_mm, auc_mm, f2_mm, prec_mm, recall_mm, best_thresh_mm, all_y_pred_probs_mm = classifier_results(X, y, MinMaxScaler(), clf)\n","        f1_ss, auc_ss, f2_ss, prec_ss, recall_ss, best_thresh_ss, all_y_pred_probs_ss = classifier_results(X, y, StandardScaler(), clf)\n","        df_result.loc[row, :] = {'Classifier':clf_name,\n","                                 'Best_threshold_MinMax':best_thresh_mm,\n","                                 'F1-score_MinMax':f1_mm, 'AUC_MinMax':auc_mm, 'F2-score_MinMax':f2_mm,\n","                                 'Precision_MinMax':prec_mm, 'Recall_MinMax':recall_mm,\n","                                 'Best_threshold_StandardScaler':best_thresh_ss,\n","                                 'F1-score_StandardScaler':f1_ss, 'AUC_StandardScaler':auc_ss, 'F2-score_StandardScaler':f2_ss,\n","                                 'Precision_StandardScaler':prec_ss, 'Recall_StandardScaler':recall_ss}\n","        row += 1\n","\n","    # To export proability\n","    if get_probability:\n","        if get_minmax:\n","            df_probs = pd.DataFrame(data=all_y_pred_probs_mm,\n","                                    columns=['Proability class 0', 'Proability class 1', 'ground truth label'])\n","            clear_output()\n","            return df_probs\n","        else:\n","            df_probs = pd.DataFrame(data=all_y_pred_probs_ss,\n","                                    columns=['Proability class 0', 'Proability class 1', 'ground truth label'])\n","            clear_output()\n","            return df_probs\n","    #################\n","    df_result['Approach'] = approach\n","    df_result['Pipline'] = pipline\n","    df_result['pretrain_weights'] = pretrain_weights\n","    df_result['embedding_structure'] = embedding_structure\n","\n","    if using_jmim:\n","        df_result['Using_jmim'] = 'Yes'\n","        return df_result.copy(), df_new_jm.copy()\n","\n","    else:\n","        df_result['Using_jmim'] = 'No'\n","        df_new['study_id'] = df['study_id']\n","        df_new['outcome'] = df['outcome']\n","        return df_result.copy(), df_new.copy()"]},{"cell_type":"code","execution_count":null,"id":"441ebcba","metadata":{"id":"441ebcba"},"outputs":[],"source":["#feature : fr\n","#oasis : os\n","#jmim : jm"]},{"cell_type":"markdown","id":"73f4a4e1","metadata":{"id":"73f4a4e1"},"source":["## Part 1: OASIS data"]},{"cell_type":"code","execution_count":null,"id":"99664c65","metadata":{"id":"99664c65","outputId":"7ae1aeb2-ec40-4a1e-a21d-4a28bdcc3fbb"},"outputs":[{"data":{"text/plain":["0    28\n","1    19\n","Name: outcome, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_os = pd.read_excel(path_data+'OASIS_ci_symptoms_6.5.23.xlsx')\n","df_os['outcome'].value_counts()"]},{"cell_type":"markdown","id":"be99163e","metadata":{"id":"be99163e"},"source":["### Without jmim"]},{"cell_type":"code","execution_count":null,"id":"8d944cbf","metadata":{"id":"8d944cbf"},"outputs":[],"source":["df_result_os, df_fr_os = classifying_all(df_os, clfs_list, clfs_name, pipline='OASIS', using_jmim=False)"]},{"cell_type":"markdown","id":"24f477d1","metadata":{"id":"24f477d1"},"source":["#### Probability"]},{"cell_type":"code","execution_count":null,"id":"f3a747d5","metadata":{"id":"f3a747d5"},"outputs":[],"source":["# df_probs_1 = classifying_all(df_os, [clfs_list[2]], [clfs_name[2]],\n","#                                         get_probability=True, get_minmax=False, pipline='OASIS', using_jmim=False)\n","# df_probs_1.to_excel(path_results+'Best_probability_for_each_approach/OASIS_probs.xlsx', index=False)"]},{"cell_type":"markdown","id":"63e23ff1","metadata":{"id":"63e23ff1"},"source":["### With jmim"]},{"cell_type":"code","execution_count":null,"id":"e5265ba2","metadata":{"scrolled":false,"id":"e5265ba2"},"outputs":[],"source":["df_result_os_jm, df_fr_os_jm = classifying_all(df_os, clfs_list, clfs_name,\n","                                               pipline='OASIS', using_jmim=True, stan_scal=True, unit_vector=True)"]},{"cell_type":"code","execution_count":null,"id":"0d80b641","metadata":{"scrolled":true,"id":"0d80b641"},"outputs":[],"source":["df_jm_oasis = classifying_all(df_os, clfs_list, clfs_name, pipline='OASIS', using_jmim=True,\n","                                               stan_scal=True, unit_vector=True, most_important_features=True)\n","df_jm_oasis.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/OASIS_JMIM_features.xlsx', index=False)"]},{"cell_type":"markdown","id":"40cbcc17","metadata":{"id":"40cbcc17"},"source":["### Aggregate"]},{"cell_type":"code","execution_count":null,"id":"036712b3","metadata":{"id":"036712b3"},"outputs":[],"source":["df_result_oasis_concat = pd.concat([df_result_os, df_result_os_jm]).reset_index(drop=True)\\\n","                                                .drop(['pretrain_weights', 'embedding_structure', 'Approach'], axis=1)\n","df_result_oasis_concat.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/1_OASIS.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"41ed55d2","metadata":{"id":"41ed55d2"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"86cc7a79","metadata":{"id":"86cc7a79"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"id":"2cfc9755","metadata":{"id":"2cfc9755"},"outputs":[],"source":["def preprocessing_text(df_text):\n","    # remove punctuation marks\n","    df_text['clean_text'] = df_text['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n","\n","    punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n","\n","    df_text['clean_text'] = df_text['clean_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n","\n","    # convert text to lowercase\n","    df_text['clean_text'] = df_text['clean_text'].str.lower()\n","\n","    # remove numbers\n","    df_text['clean_text'] = df_text['clean_text'].str.replace(\"[0-9]\", \" \")\n","\n","    # remove whitespaces\n","    df_text['clean_text'] = df_text['clean_text'].apply(lambda x:' '.join(x.split()))\n","\n","    return df_text"]},{"cell_type":"code","execution_count":null,"id":"13d6a4c2","metadata":{"id":"13d6a4c2"},"outputs":[],"source":["def classifying_using_bert(df, clfs_list, clfs_name, approach, pretrain_weights, embedding_structure,\n","                           using_jmim, unit_vector=False, clinical=False, nurse=False, n_features_jmim='auto'):\n","    ##############################################\n","    SEED = 55\n","    transformers.set_seed(SEED)\n","    random.seed(SEED)\n","    os.environ['PYTHONHASHSEED'] = str(SEED)\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","    #############################################\n","\n","    # filling hosp_ed_ind of 1010 study_id with 0\n","    df = preprocessing_text(df.copy()).copy()\n","    # BERT\n","    model = AutoModel.from_pretrained(pretrain_weights)\n","    tokenizer = AutoTokenizer.from_pretrained(pretrain_weights)\n","    tokenized = df[\"clean_text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True, )))\n","\n","    max_len = 512\n","    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n","    attention_mask = np.where(padded != 0, 1, 0)\n","\n","    input_ids1 = torch.tensor(padded)\n","    attention_mask1 = torch.tensor(attention_mask)\n","\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    last_hidden_states1 = model(input_ids1, attention_mask=attention_mask1)\n","    lhs1 = last_hidden_states1[0].numpy()\n","\n","    if embedding_structure == 'mean':\n","        X = np.mean(lhs1, axis=1)\n","\n","    elif embedding_structure == 'first':\n","        X = lhs1[:, 0, :]\n","\n","    if nurse:\n","        add_person_col = '_ns'\n","    else:\n","        add_person_col = '_pt'\n","\n","    if clinical:\n","        add_data_col = '_clinical'\n","    else:\n","        add_data_col = '_transcription'\n","\n","    df_embedding = pd.DataFrame(data=X, columns=[\"embedding_dim_\" + str(i) + add_data_col + add_person_col  for i in range(X.shape[1])])\n","    df_embedding['study_id'] = df[\"study_id\"]\n","    df_embedding['outcome'] = df['outcome']\n","    y = df['outcome'].to_numpy()\n","\n","    if using_jmim:\n","        t = MinMaxScaler()\n","        # t = StandardScaler()\n","        X_t = t.fit_transform(X)\n","\n","        if unit_vector:\n","            ratio = np.sqrt(np.sum(np.square(X_t), axis=1))\n","            ratio = ratio.reshape(ratio.shape[0], 1)\n","            X_t = X_t/ratio\n","\n","        MIFS = mifs.MutualInformationFeatureSelector(method='JMIM', k=5, n_features=n_features_jmim, verbose=2)\n","        MIFS.fit_transform(X_t, df['outcome'])\n","\n","        df_embed_jm = df_embedding.drop(['study_id', 'outcome'], axis=1).iloc[:, MIFS.ranking_]\n","\n","        X = df_embed_jm.to_numpy()\n","        df_embed_jm['study_id'] = df_embedding['study_id']\n","        df_embed_jm['outcome'] = df_embedding['outcome']\n","\n","    row = 0\n","    for clf, clf_name in zip(clfs_list, clfs_name):\n","        f1_mm, auc_mm, f2_mm, prec_mm, recall_mm, best_thresh_mm, all_y_pred_probs_mm = classifier_results(X, y, MinMaxScaler(), clf)\n","        f1_ss, auc_ss, f2_ss, prec_ss, recall_ss, best_thresh_ss, all_y_pred_probs_ss = classifier_results(X, y, StandardScaler(), clf)\n","        df_result.loc[row, :] = {'Classifier':clf_name,\n","                                 'Best_threshold_MinMax':best_thresh_mm,\n","                                 'F1-score_MinMax':f1_mm, 'AUC_MinMax':auc_mm, 'F2-score_MinMax':f2_mm,\n","                                 'Precision_MinMax':prec_mm, 'Recall_MinMax':recall_mm,\n","                                 'Best_threshold_StandardScaler':best_thresh_ss,\n","                                 'F1-score_StandardScaler':f1_ss, 'AUC_StandardScaler':auc_ss, 'F2-score_StandardScaler':f2_ss,\n","                                 'Precision_StandardScaler':prec_ss, 'Recall_StandardScaler':recall_ss}\n","        row += 1\n","    ####################\n","    df_result['Approach'] = approach\n","    df_result['pretrain_weights'] = pretrain_weights\n","    df_result['embedding_structure'] = embedding_structure\n","\n","    if using_jmim:\n","        df_result['Using_jmim'] = 'Yes'\n","        return df_result.copy(), df_embed_jm.copy()\n","    else:\n","        df_result['Using_jmim'] = 'No'\n","        return df_result.copy(), df_embedding.copy()\n",""]},{"cell_type":"code","execution_count":null,"id":"c8bc08cf","metadata":{"id":"c8bc08cf"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7a2b86bb","metadata":{"id":"7a2b86bb"},"source":["## Load clinical note:"]},{"cell_type":"code","execution_count":null,"id":"a7b8ad77","metadata":{"id":"a7b8ad77"},"outputs":[],"source":["df_last_clinic = pd.read_excel(path+'clinical_note_final_data/last_datetime_clinical_note_of_each_patient.xlsx')\n","df_last_clinical = df_last_clinic.set_index('study_id').drop(1064).reset_index()\n","df_last_clinical = pd.merge(df_last_clinical, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_clinic = pd.read_excel(path+'clinical_note_final_data/combining_clinical_note_of_datetimes_for_each_patient.xlsx')\n","df_comb_clinical = df_comb_clinic.set_index('study_id').drop(1064).reset_index()\n","df_comb_clinical = pd.merge(df_comb_clinical, df_os[['study_id', 'outcome']], on='study_id')"]},{"cell_type":"markdown","id":"1c4ea931","metadata":{"id":"1c4ea931"},"source":["## Load patient data:\n","## transcription + UMLS + LIWC + Turn taking + Linguistic"]},{"cell_type":"code","execution_count":null,"id":"ae89d40b","metadata":{"id":"ae89d40b"},"outputs":[],"source":["df_last_transcription = pd.read_excel(path+'transcription_final_data/patient/last_datetime_text_of_each_patient.xlsx')\n","df_last_transcription = df_last_transcription.set_index('study_id').drop(1022).reset_index()\n","df_last_transcription = pd.merge(df_last_transcription, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_transcription = pd.read_excel(path+'transcription_final_data/patient/combining_text_of_datetimes_for_each_patient.xlsx')\n","df_comb_transcription = df_comb_transcription.set_index('study_id').drop(1022).reset_index()\n","df_comb_transcription = pd.merge(df_comb_transcription, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_umls = pd.read_excel(path+'transcription_final_data/patient/last_datetime_umls_of_each_patient.xlsx')\n","df_last_umls = df_last_umls.set_index('study_id').drop(1022).reset_index()\n","df_last_umls = pd.merge(df_last_umls, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_umls = pd.read_excel(path+'transcription_final_data/patient/combining_umls_of_datetimes_for_each_patient.xlsx')\n","df_comb_umls = df_comb_umls.set_index('study_id').drop(1022).reset_index()\n","df_comb_umls = pd.merge(df_comb_umls, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_liwc = pd.read_excel(path+'transcription_final_data/patient/LIWC2015 Results (last_datetime_text_of_each_patient).xlsx')\n","df_last_liwc = df_last_liwc.rename(columns={'Source (A)':'study_id', 'Source (B)':'text', 'Source (C)':'hosp_ed_ind'})\n","df_last_liwc = df_last_liwc.set_index('study_id').drop(1022).reset_index()\n","df_last_liwc = pd.merge(df_last_liwc, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_liwc = pd.read_excel(path+'transcription_final_data/patient/LIWC2015 Results (combining_text_of_datetimes_for_each_patient).xlsx')\n","df_comb_liwc = df_comb_liwc.rename(columns={'Source (A)':'study_id', 'Source (B)':'hosp_ed_ind', 'Source (C)':'text'})\n","df_comb_liwc = df_comb_liwc.set_index('study_id').drop(1022).reset_index()\n","df_comb_liwc = pd.merge(df_comb_liwc, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_turn = pd.read_excel(path+'transcription_final_data/patient/last_datetime_related_turn_taking_features_of_each_patient.xlsx')\n","df_last_turn = df_last_turn.set_index('study_id').drop(1022).reset_index()\n","df_last_turn = pd.merge(df_last_turn, df_os[['study_id', 'outcome']], on='study_id')\n","df_mean_turn = pd.read_excel(path+'transcription_final_data/patient/mean_datetime_related_turn_taking_features_of_each_patient.xlsx')\n","df_mean_turn = df_mean_turn.set_index('study_id').drop(1022).reset_index()\n","df_mean_turn = pd.merge(df_mean_turn, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_ling = pd.read_excel(path_data+'Linguistic_features_last_date_time_cognitive_status_symptoms.xlsx')\n","df_comb_ling = pd.read_excel(path_data+'Linguistic_features_combine_text_cognitive_status_symptoms.xlsx')"]},{"cell_type":"markdown","id":"a938c251","metadata":{"id":"a938c251"},"source":["### UMLS"]},{"cell_type":"markdown","id":"35233621","metadata":{"id":"35233621"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"81f0cef8","metadata":{"id":"81f0cef8"},"outputs":[],"source":["df_result_last_umls_jm, df_fr_last_umls_jm = classifying_all(df_last_umls, clfs_list, clfs_name,\n","                                                pipline='UMLS', using_jmim=True, stan_scal=True, unit_vector=True)"]},{"cell_type":"markdown","id":"4ebd1409","metadata":{"id":"4ebd1409"},"source":["#### combine text"]},{"cell_type":"code","execution_count":null,"id":"73bf8f9a","metadata":{"id":"73bf8f9a"},"outputs":[],"source":["df_result_comb_umls_jm, df_fr_comb_umls_jm = classifying_all(df_comb_umls, clfs_list, clfs_name,\n","                                                pipline='UMLS', using_jmim=True, stan_scal=True, unit_vector=True)"]},{"cell_type":"markdown","id":"a4c1e08d","metadata":{"id":"a4c1e08d"},"source":["### LIWC"]},{"cell_type":"markdown","id":"40d84cfc","metadata":{"id":"40d84cfc"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"2ad6209d","metadata":{"id":"2ad6209d"},"outputs":[],"source":["df_result_last_liwc_jm, df_fr_last_liwc_jm = classifying_all(df_last_liwc, clfs_list, clfs_name,\n","                                                                         pipline='LIWC', using_jmim=True)"]},{"cell_type":"markdown","id":"6e28f400","metadata":{"id":"6e28f400"},"source":["#### combine text"]},{"cell_type":"code","execution_count":null,"id":"caef4af6","metadata":{"id":"caef4af6"},"outputs":[],"source":["df_result_comb_liwc_jm, df_fr_comb_liwc_jm = classifying_all(df_comb_liwc, clfs_list, clfs_name,\n","                                                             pipline='LIWC', using_jmim=True)"]},{"cell_type":"code","execution_count":null,"id":"f2cc759b","metadata":{"id":"f2cc759b"},"outputs":[],"source":["df_comb_liwc_jmim = classifying_all(df_comb_liwc, clfs_list, clfs_name,\n","                                        pipline='LIWC', using_jmim=True, most_important_features=True)\n","df_comb_liwc_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/LIWC_JMIM_features_for_combined_text_pt.xlsx', index=False)"]},{"cell_type":"markdown","id":"df1b7554","metadata":{"id":"df1b7554"},"source":["### Turn taking"]},{"cell_type":"markdown","id":"fdc4b842","metadata":{"id":"fdc4b842"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"125d2cef","metadata":{"id":"125d2cef"},"outputs":[],"source":["df_result_last_turn_jm, df_fr_last_turn_jm = classifying_all(df_last_turn, clfs_list, clfs_name,\n","                                                             pipline='Turn_taking', using_jmim=True)"]},{"cell_type":"markdown","id":"2f9f5cf6","metadata":{"id":"2f9f5cf6"},"source":["#### mean date time"]},{"cell_type":"code","execution_count":null,"id":"aefa696b","metadata":{"id":"aefa696b"},"outputs":[],"source":["df_result_mean_turn_jm, df_fr_mean_turn_jm = classifying_all(df_mean_turn, clfs_list, clfs_name,\n","                                                             pipline='Turn_taking', using_jmim=True)"]},{"cell_type":"code","execution_count":null,"id":"58fd0e51","metadata":{"id":"58fd0e51"},"outputs":[],"source":["df_mean_turn_jmim = classifying_all(df_mean_turn, clfs_list, clfs_name,\n","                                    pipline='Turn_taking', using_jmim=True, most_important_features=True)\n","df_mean_turn_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/Turn_taking_JMIM_features_for_mean_pt.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"60e1fe26","metadata":{"id":"60e1fe26"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a9685262","metadata":{"id":"a9685262"},"source":["### Linguistic features"]},{"cell_type":"markdown","id":"131afce4","metadata":{"id":"131afce4"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"596db0f2","metadata":{"id":"596db0f2"},"outputs":[],"source":["df_result_last_ling_jm, df_fr_last_ling_jm = classifying_all(df_last_ling, clfs_list, clfs_name,\n","                                            pipline='Linguistic', using_jmim=True)"]},{"cell_type":"markdown","id":"6c1811a8","metadata":{"id":"6c1811a8"},"source":["#### combine text"]},{"cell_type":"code","execution_count":null,"id":"7378a49a","metadata":{"id":"7378a49a"},"outputs":[],"source":["df_result_comb_ling_jm, df_fr_comb_ling_jm = classifying_all(df_comb_ling, clfs_list, clfs_name,\n","                                            pipline='Linguistic', using_jmim=True)"]},{"cell_type":"code","execution_count":null,"id":"86bc1a9e","metadata":{"id":"86bc1a9e"},"outputs":[],"source":["df_comb_ling_jmim = classifying_all(df_comb_ling, clfs_list, clfs_name,\n","                                    pipline='Linguistic', using_jmim=True, most_important_features=True)\n","df_comb_ling_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/Linguistic_JMIM_features_for_combined_text_pt.xlsx', index=False)"]},{"cell_type":"markdown","id":"217a5aa5","metadata":{"id":"217a5aa5"},"source":["### Aggregate features\n"]},{"cell_type":"code","execution_count":null,"id":"27a45ba2","metadata":{"id":"27a45ba2"},"outputs":[],"source":["all_df_fr_last_jm = [df_fr_last_ling_jm, df_fr_last_turn_jm, df_fr_last_liwc_jm, df_fr_last_umls_jm]\n","# all_df_fr_last_jm = [df_fr_last_turn_jm, df_fr_last_liwc_jm]\n","df_fr_last_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_df_fr_last_jm)"]},{"cell_type":"code","execution_count":null,"id":"98a69785","metadata":{"id":"98a69785"},"outputs":[],"source":["all_without_umls_df_fr_last_jm = [df_fr_last_ling_jm, df_fr_last_turn_jm, df_fr_last_liwc_jm]\n","# all_df_fr_last_jm = [df_fr_last_turn_jm, df_fr_last_liwc_jm]\n","df_without_umls_fr_last_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_without_umls_df_fr_last_jm)"]},{"cell_type":"code","execution_count":null,"id":"14e3f49b","metadata":{"id":"14e3f49b"},"outputs":[],"source":["all_df_fr_comb_jm = [df_fr_comb_ling_jm, df_fr_mean_turn_jm, df_fr_comb_liwc_jm, df_fr_comb_umls_jm]\n","# all_df_fr_comb_jm = [df_fr_mean_turn_jm, df_fr_comb_liwc_jm]\n","df_fr_comb_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_df_fr_comb_jm)"]},{"cell_type":"code","execution_count":null,"id":"0a96b6f5","metadata":{"id":"0a96b6f5"},"outputs":[],"source":["all_without_umls_df_fr_comb_jm = [df_fr_comb_ling_jm, df_fr_mean_turn_jm, df_fr_comb_liwc_jm]\n","# all_df_fr_comb_jm = [df_fr_mean_turn_jm, df_fr_comb_liwc_jm]≥\n","df_without_umls_fr_comb_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_without_umls_df_fr_comb_jm)"]},{"cell_type":"markdown","id":"662f2dfc","metadata":{"id":"662f2dfc"},"source":["### last date time"]},{"cell_type":"markdown","id":"709925a8","metadata":{"id":"709925a8"},"source":["#### without jmim"]},{"cell_type":"code","execution_count":null,"id":"888d40eb","metadata":{"id":"888d40eb"},"outputs":[],"source":["df_result_4_last, df_fr_last_4 = classifying_all(df_fr_last_jm, clfs_list, clfs_name,\n","    approach='last datetime text', pipline='Linguistic_jmim + Turn_taking_jmim + LIWC_jmim + UMLS_jmim', using_jmim=False)"]},{"cell_type":"markdown","id":"27abb5a1","metadata":{"id":"27abb5a1"},"source":["#### with jmim"]},{"cell_type":"code","execution_count":null,"id":"67eef8af","metadata":{"id":"67eef8af"},"outputs":[],"source":["df_result_4_last_jm, df_fr_last_4_jm = classifying_all(df_fr_last_jm, clfs_list, clfs_name,\n","    approach='last datetime text', pipline='Linguistic_jmim + Turn_taking_jmim + LIWC_jmim + UMLS_jmim', using_jmim=True)"]},{"cell_type":"markdown","id":"84ec07cb","metadata":{"id":"84ec07cb"},"source":["### Combine text"]},{"cell_type":"markdown","id":"a3094add","metadata":{"id":"a3094add"},"source":["#### without jmim"]},{"cell_type":"code","execution_count":null,"id":"a09b7635","metadata":{"id":"a09b7635"},"outputs":[],"source":["df_result_4_comb, df_fr_comb_4 = classifying_all(df_fr_comb_jm, clfs_list, clfs_name,\n","    approach='combine text', pipline='Linguistic_jmim + Turn_taking_jmim + LIWC_jmim + UMLS_jmim', using_jmim=False)"]},{"cell_type":"markdown","id":"bd4426cb","metadata":{"id":"bd4426cb"},"source":["#### with jmim"]},{"cell_type":"code","execution_count":null,"id":"ca7a85fe","metadata":{"id":"ca7a85fe"},"outputs":[],"source":["df_result_4_comb_jm, df_fr_comb_4_jm = classifying_all(df_fr_comb_jm, clfs_list, clfs_name,\n","    approach='combine text', pipline='Linguistic_jmim + Turn_taking_jmim + LIWC_jmim + UMLS_jmim', using_jmim=True)"]},{"cell_type":"markdown","id":"4d21234b","metadata":{"id":"4d21234b"},"source":["### Aggregate results"]},{"cell_type":"code","execution_count":null,"id":"67fcebfd","metadata":{"id":"67fcebfd"},"outputs":[],"source":["df_result_4_set = pd.concat([df_result_4_last, df_result_4_last_jm, df_result_4_comb, df_result_4_comb_jm])\\\n","                            .drop(['pretrain_weights', 'embedding_structure'], axis=1).reset_index(drop=True)\n","df_result_4_set.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Linguistic_Turn_taking_jmim_LIWC_jmim_UMLS_jmim.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"e618b373","metadata":{"id":"e618b373"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b6022d83","metadata":{"id":"b6022d83"},"source":["## Part 2: OASIS + Clinical note"]},{"cell_type":"code","execution_count":null,"id":"089b9aa4","metadata":{"id":"089b9aa4"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"efd294d8","metadata":{"id":"efd294d8"},"outputs":[],"source":["try:\n","    df_result_last_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/2_OASIS_Clinical_note.xlsx')\n","    last_index = df_result_last_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_last_all.loc[last_index, 'Approach']\n","    second = df_result_last_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_last_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_last_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except FileNotFoundError:\n","    run_again_idx = 0\n","    df_result_last_all = pd.DataFrame()\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_clinical = df_last_clinical.copy()\n","    elif episode[0]=='combined text':\n","        df_clinical = df_comb_clinical.copy()\n","\n","    df_result_bert_jm, df_fr_bert_jm = classifying_using_bert(df_clinical, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","\n","    df_fr_jm = pd.merge(df_fr_os_jm, df_fr_bert_jm, on=['study_id', 'outcome'])\n","\n","    df_result_final, df_fr_final = classifying_all(df_fr_jm, clfs_list, clfs_name,\n","                               approach=episode[0], pipline='oasis_jmim + transformer_jmim',\n","                               pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","\n","    df_result_last_all = pd.concat([df_result_last_all, df_result_final]).reset_index(drop=True)\n","    df_result_last_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/2_OASIS_Clinical_note.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"0a59dde6","metadata":{"id":"0a59dde6"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ebee5e9b","metadata":{"id":"ebee5e9b"},"source":["### Best model probabilities of part 2"]},{"cell_type":"code","execution_count":null,"id":"3b0c653e","metadata":{"id":"3b0c653e","outputId":"930a974d-ac4a-4526-b66d-6ddbfc538db1"},"outputs":[{"data":{"text/plain":["('combined text', True, 'bert-base-uncased', 'mean')"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["episodes[19]"]},{"cell_type":"code","execution_count":null,"id":"ed31b45d","metadata":{"id":"ed31b45d"},"outputs":[],"source":["episode = episodes[19]\n","\n","print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                  .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","if episode[0]=='last date time':\n","    df_clinical = df_last_clinical.copy()\n","elif episode[0]=='combined text':\n","    df_clinical = df_comb_clinical.copy()\n","\n","df_result_bert_jm, df_fr_bert_jm = classifying_using_bert(df_clinical, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True)\n","\n","df_fr_jm = pd.merge(df_fr_os_jm, df_fr_bert_jm, on=['study_id', 'outcome'])\n","\n","##############\n","df_probs_combined_2 = classifying_all(df_fr_jm, [clfs_list[3]], [clfs_name[3]],\n","        get_probability=True, get_minmax=False,\n","        approach=episode[0], pipline='oasis_jmim + transformer_jmim',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])"]},{"cell_type":"code","execution_count":null,"id":"c87e359c","metadata":{"id":"c87e359c"},"outputs":[],"source":["df_probs_combined_2.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Best_probability_for_each_approach/2_OASIS_Clinical_note_combined_text_probs.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"555aca7f","metadata":{"id":"555aca7f"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"af1f4218","metadata":{"id":"af1f4218"},"source":["## Part 3:\n","## patient: (transformer on transcription + LIWC + Turn taking + Linguistic)"]},{"cell_type":"code","execution_count":null,"id":"e2503395","metadata":{"id":"e2503395"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"722b7fbc","metadata":{"scrolled":true,"id":"722b7fbc"},"outputs":[],"source":["try:\n","    df_result_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/3_Linguistic_Turn_taking_LIWC_Transcription.xlsx')\n","    last_index = df_result_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_all.loc[last_index, 'Approach']\n","    second = df_result_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except FileNotFoundError:\n","    df_result_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_transcription = df_last_transcription.copy()\n","        df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_last_jm.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_transcription = df_comb_transcription.copy()\n","        df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_comb_jm.copy()\n","    ##############\n","    df_result_bert_jm, df_fr_bert_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    dataframes = [df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_bert_jm]\n","    df_fr_4set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_1, df_fr_4set_jm_no_yes_jm = classifying_all(df_fr_4set_jm, clfs_list, clfs_name,\n","            approach=episode[0], pipline='linguistic_jmim + turn_taking_jmim + liwc_jmim '\n","                                                                 '+ transformer_on_transcription_jmim',\n","            pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    # 3_1 -> 1(3) + 1\n","    df_fr_1_3set_jm = pd.merge(df_frs_jm, df_fr_bert_jm, on=['study_id', 'outcome'])\n","\n","    df_result_final_2, df_fr_1_3set_jm_no_yes_jm = classifying_all(df_fr_1_3set_jm, clfs_list, clfs_name,\n","            approach=episode[0], pipline='(linguistic_jmim + turn_taking_jmim + liwc_jmim)jmim '\n","                                                                   '+ transformer_on_transcription_jmim',\n","            pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    df_result_all = pd.concat([df_result_all, df_result_final_1, df_result_final_2]).reset_index(drop=True)\n","    df_result_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/3_Linguistic_Turn_taking_LIWC_Transcription.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"1a93f098","metadata":{"id":"1a93f098"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"60723858","metadata":{"id":"60723858"},"source":["### Best model probabilities of part 3"]},{"cell_type":"code","execution_count":null,"id":"0f24f192","metadata":{"id":"0f24f192","outputId":"a9e081b4-5031-4f88-c370-81fc0f033fc0"},"outputs":[{"data":{"text/plain":["('combined text', True, 'distilbert-base-uncased', 'mean')"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["episodes[21]"]},{"cell_type":"code","execution_count":null,"id":"8f4b6f60","metadata":{"id":"8f4b6f60"},"outputs":[],"source":["episode = episodes[21]\n","\n","print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                  .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","if episode[0]=='last date time':\n","    df_transcription = df_last_transcription.copy()\n","    df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","    df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","    df_frs_jm = df_without_umls_fr_last_jm.copy()\n","\n","elif episode[0]=='combined text':\n","    df_transcription = df_comb_transcription.copy()\n","    df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","    df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","    df_frs_jm = df_without_umls_fr_comb_jm.copy()\n","##############\n","df_result_bert_jm, df_fr_bert_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True)\n","\n","##############\n","# 3_1 -> 1(3) + 1\n","df_fr_1_3set_jm = pd.merge(df_frs_jm, df_fr_bert_jm, on=['study_id', 'outcome'])\n","\n","##############\n","df_probs_combined_3 = classifying_all(df_fr_1_3set_jm, [clfs_list[1]], [clfs_name[1]],\n","        get_probability=True, get_minmax=False,\n","        approach=episode[0], pipline='(linguistic_jmim + turn_taking_jmim + liwc_jmim)jmim '\n","                                                               '+ transformer_on_transcription_jmim',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n"]},{"cell_type":"code","execution_count":null,"id":"fffa7587","metadata":{"id":"fffa7587"},"outputs":[],"source":["df_probs_combined_3.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Best_probability_for_each_approach/3_Linguistic_Turn_taking_LIWC_Transcription_combined_text_probs.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"4b334f70","metadata":{"id":"4b334f70"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"db75484c","metadata":{"id":"db75484c"},"source":["## Part 3_1:\n","## patient: transformer on transcription"]},{"cell_type":"code","execution_count":null,"id":"5732d39e","metadata":{"id":"5732d39e"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"b9bcf364","metadata":{"scrolled":true,"id":"b9bcf364"},"outputs":[],"source":["try:\n","    df_result_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/3.1_transcription_pt.xlsx')\n","    last_index = df_result_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_all.loc[last_index, 'Approach']\n","    second = df_result_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except FileNotFoundError:\n","    df_result_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_transcription = df_last_transcription.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_transcription = df_comb_transcription.copy()\n","\n","\n","    ##############\n","    df_result_bert_jm, df_fr_bert_jm = classifying_using_bert(df_transcription, [clfs_list[3]], [clfs_name[3]],\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=episode[1])\n","\n","    ##############\n","    df_result_all = pd.concat([df_result_all, df_result_bert_jm]).reset_index(drop=True)\n","    df_result_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/3.1_transcription_pt.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"3314ae41","metadata":{"id":"3314ae41"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"6d959f0b","metadata":{"id":"6d959f0b"},"source":["## Part 4:\n","## patient: (OASIS + Clinical note) +\n","## patient: (transformer on transcription + LIWC + Turn taking + Linguistic)"]},{"cell_type":"code","execution_count":null,"id":"519657e1","metadata":{"id":"519657e1"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"4fc1afd5","metadata":{"id":"4fc1afd5"},"outputs":[],"source":["try:\n","    df_result_last_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/4_OASIS_Linguistic_Turn_taking_LIWC_Transcription_Clinical.xlsx')\n","    last_index = df_result_last_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_last_all.loc[last_index, 'Approach']\n","    second = df_result_last_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_last_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_last_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except FileNotFoundError:\n","    df_result_last_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_clinical = df_last_clinical.copy()\n","        df_transcription = df_last_transcription.copy()\n","        df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_last_jm.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_clinical = df_comb_clinical.copy()\n","        df_transcription = df_comb_transcription.copy()\n","        df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_comb_jm.copy()\n","\n","    ##############\n","    df_result_bert_transc_jm, df_fr_bert_transc_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    df_result_bert_clinic_jm, df_fr_bert_clinic_jm = classifying_using_bert(df_clinical, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    dataframes = [df_fr_os_jm, df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_bert_clinic_jm, df_fr_bert_transc_jm]\n","    df_fr_6set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_1, df_fr_6set_jm_no_yes_jm = classifying_all(df_fr_6set_jm, clfs_list, clfs_name,\n","            approach=episode[0], pipline='oasis_jmim + linguistic_jmim + turn_taking_jmim + liwc_jmim + transformer_on_clinical_jmim + transformer_on_transcription_jmim',\n","            pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    # 1_3_1_1 -> 1 + 1(3) + 1 + 1\n","    dataframes = [df_fr_os_jm, df_frs_jm, df_fr_bert_clinic_jm, df_fr_bert_transc_jm]\n","    df_fr_1_3_1_1set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_2, df_fr_1_3_1_1set_jm_no_yes_jm = classifying_all(df_fr_1_3_1_1set_jm, clfs_list, clfs_name,\n","            approach=episode[0], pipline='oasis_jmim + (linguistic_jmim + turn_taking_jmim + liwc_jmim)jmim + transformer_on_clinical_jmim + transformer_on_transcription_jmim',\n","            pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    df_result_last_all = pd.concat([df_result_last_all, df_result_final_1, df_result_final_2]).reset_index(drop=True)\n","    df_result_last_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/4_OASIS_Linguistic_Turn_taking_LIWC_Transcription_Clinical.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"e66cb1d5","metadata":{"id":"e66cb1d5"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"517b7bae","metadata":{"id":"517b7bae"},"source":["## Load nurse data:\n","## transcription + UMLS + LIWC"]},{"cell_type":"code","execution_count":null,"id":"b6492d2b","metadata":{"id":"b6492d2b"},"outputs":[],"source":["df_last_transcription_ns = pd.read_excel(path+'transcription_final_data/nurse/last_datetime_text_of_each_nurse.xlsx')\n","df_last_transcription_ns = df_last_transcription_ns.set_index('study_id').drop(1022).reset_index()\n","df_last_transcription_ns = pd.merge(df_last_transcription_ns, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_transcription_ns = pd.read_excel(path+'transcription_final_data/nurse/combining_text_of_datetimes_for_each_nurse.xlsx')\n","df_comb_transcription_ns = df_comb_transcription_ns.set_index('study_id').drop(1022).reset_index()\n","df_comb_transcription_ns = pd.merge(df_comb_transcription_ns, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_umls_ns = pd.read_excel(path+'transcription_final_data/nurse/last_datetime_umls_of_each_nurse.xlsx')\n","df_last_umls_ns = df_last_umls_ns.set_index('study_id').drop(1022).reset_index()\n","df_last_umls_ns = pd.merge(df_last_umls_ns, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_umls_ns = pd.read_excel(path+'transcription_final_data/nurse/combining_umls_of_datetimes_for_each_nurse.xlsx')\n","df_comb_umls_ns = df_comb_umls_ns.set_index('study_id').drop(1022).reset_index()\n","df_comb_umls_ns = pd.merge(df_comb_umls_ns, df_os[['study_id', 'outcome']], on='study_id')\n","\n","df_last_liwc_ns = pd.read_excel(path+'transcription_final_data/nurse/LIWC2015 Results (last_datetime_text_of_each_nurse).xlsx')\n","df_last_liwc_ns = df_last_liwc_ns.rename(columns={'Source (A)':'study_id', 'Source (B)':'date_time', 'Source (C)':'text', 'Source (D)':'hosp_ed_ind'})\n","df_last_liwc_ns = df_last_liwc_ns.set_index('study_id').drop(1022).reset_index()\n","df_last_liwc_ns = pd.merge(df_last_liwc_ns, df_os[['study_id', 'outcome']], on='study_id')\n","df_comb_liwc_ns = pd.read_excel(path+'transcription_final_data/nurse/LIWC2015 Results (combining_text_of_datetimes_for_each_nurse).xlsx')\n","df_comb_liwc_ns = df_comb_liwc_ns.rename(columns={'Source (A)':'study_id', 'Source (B)':'hosp_ed_ind', 'Source (C)':'text'})\n","df_comb_liwc_ns = df_comb_liwc_ns.set_index('study_id').drop(1022).reset_index()\n","df_comb_liwc_ns = pd.merge(df_comb_liwc_ns, df_os[['study_id', 'outcome']], on='study_id')\n"]},{"cell_type":"markdown","id":"c77f866b","metadata":{"id":"c77f866b"},"source":["### UMLS"]},{"cell_type":"markdown","id":"b64abbb9","metadata":{"id":"b64abbb9"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"e5898474","metadata":{"id":"e5898474"},"outputs":[],"source":["df_result_last_umls_jm_ns, df_fr_last_umls_jm_ns = classifying_all(df_last_umls_ns, clfs_list, clfs_name,\n","                                    pipline='UMLS', using_jmim=True, stan_scal=True, unit_vector=True, nurse=True)"]},{"cell_type":"markdown","id":"d651b086","metadata":{"id":"d651b086"},"source":["#### combine text"]},{"cell_type":"code","execution_count":null,"id":"2eff8c8b","metadata":{"id":"2eff8c8b"},"outputs":[],"source":["df_result_comb_umls_jm_ns, df_fr_comb_umls_jm_ns = classifying_all(df_comb_umls_ns, clfs_list, clfs_name,\n","                                    pipline='UMLS', using_jmim=True, stan_scal=True, unit_vector=True, nurse=True)"]},{"cell_type":"markdown","id":"00f107ee","metadata":{"id":"00f107ee"},"source":["### LIWC"]},{"cell_type":"markdown","id":"80293408","metadata":{"id":"80293408"},"source":["#### last date time"]},{"cell_type":"code","execution_count":null,"id":"f8b8f2be","metadata":{"id":"f8b8f2be"},"outputs":[],"source":["df_result_last_liwc_jm_ns, df_fr_last_liwc_jm_ns = classifying_all(df_last_liwc_ns, clfs_list, clfs_name,\n","                                                                   pipline='LIWC', using_jmim=True, nurse=True)"]},{"cell_type":"markdown","id":"83f8825c","metadata":{"id":"83f8825c"},"source":["#### combine text"]},{"cell_type":"code","execution_count":null,"id":"89a0a5fa","metadata":{"id":"89a0a5fa"},"outputs":[],"source":["df_result_comb_liwc_jm_ns, df_fr_comb_liwc_jm_ns = classifying_all(df_comb_liwc_ns, clfs_list, clfs_name,\n","                                                                   pipline='LIWC', using_jmim=True, nurse=True)"]},{"cell_type":"code","execution_count":null,"id":"31ff7e3d","metadata":{"id":"31ff7e3d"},"outputs":[],"source":["df_comb_liwc_jmim_ns = classifying_all(df_comb_liwc_ns, clfs_list, clfs_name,\n","                                    pipline='LIWC', using_jmim=True, nurse=True, most_important_features=True)\n","df_comb_liwc_jmim_ns.to_excel(path_results+'nurse/One_leave_out/Respond_to_reviewers/Most_important_features/LIWC_JMIM_features_for_combined_text_ns.xlsx', index=False)"]},{"cell_type":"markdown","id":"a257823b","metadata":{"id":"a257823b"},"source":["### Aggregate features\n"]},{"cell_type":"code","execution_count":null,"id":"7789a82a","metadata":{"id":"7789a82a"},"outputs":[],"source":["all_df_fr_last_jm_ns = [df_fr_last_liwc_jm_ns, df_fr_last_umls_jm_ns]\n","df_fr_last_jm_ns = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_df_fr_last_jm_ns)"]},{"cell_type":"code","execution_count":null,"id":"f394a455","metadata":{"id":"f394a455"},"outputs":[],"source":["all_df_fr_comb_jm_ns = [df_fr_comb_liwc_jm_ns, df_fr_comb_umls_jm_ns]\n","df_fr_comb_jm_ns = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), all_df_fr_comb_jm_ns)\n"]},{"cell_type":"markdown","id":"37c8a250","metadata":{"id":"37c8a250"},"source":["### last date time"]},{"cell_type":"markdown","id":"3f3acafa","metadata":{"id":"3f3acafa"},"source":["#### without jmim"]},{"cell_type":"code","execution_count":null,"id":"bdbc3fbc","metadata":{"id":"bdbc3fbc"},"outputs":[],"source":["df_result_2_last_ns, df_fr_last_2_ns = classifying_all(df_fr_last_jm_ns, clfs_list, clfs_name,\n","            approach='last datetime text', pipline='LIWC_jmim + UMLS_jmim', using_jmim=False)"]},{"cell_type":"markdown","id":"28ab0243","metadata":{"id":"28ab0243"},"source":["#### with jmim"]},{"cell_type":"code","execution_count":null,"id":"a2f772af","metadata":{"id":"a2f772af"},"outputs":[],"source":["df_result_2_last_jm_ns, df_fr_last_2_jm_ns = classifying_all(df_fr_last_jm_ns, clfs_list, clfs_name,\n","            approach='last datetime text', pipline='LIWC_jmim + UMLS_jmim', using_jmim=True)"]},{"cell_type":"markdown","id":"8bb2dd72","metadata":{"id":"8bb2dd72"},"source":["### Combine text"]},{"cell_type":"markdown","id":"503ecf77","metadata":{"id":"503ecf77"},"source":["#### without jmim"]},{"cell_type":"code","execution_count":null,"id":"a8b14011","metadata":{"id":"a8b14011"},"outputs":[],"source":["df_result_2_comb_ns, df_fr_comb_2_ns = classifying_all(df_fr_comb_jm_ns, clfs_list, clfs_name,\n","            approach='combine text', pipline='LIWC_jmim + UMLS_jmim', using_jmim=False)"]},{"cell_type":"markdown","id":"82d9019c","metadata":{"id":"82d9019c"},"source":["#### with jmim"]},{"cell_type":"code","execution_count":null,"id":"56233a16","metadata":{"id":"56233a16"},"outputs":[],"source":["df_result_2_comb_jm_ns, df_fr_comb_2_jm_ns = classifying_all(df_fr_comb_jm_ns, clfs_list, clfs_name,\n","            approach='combine text', pipline='Turn_taking_jmim + LIWC_jmim + UMLS_jmim', using_jmim=True)\n"]},{"cell_type":"markdown","id":"b2e0f0cb","metadata":{"id":"b2e0f0cb"},"source":["### Aggregate results"]},{"cell_type":"code","execution_count":null,"id":"4efa57c7","metadata":{"id":"4efa57c7"},"outputs":[],"source":["df_result_2_set = pd.concat([df_result_2_last_ns, df_result_2_last_jm_ns, df_result_2_comb_ns, df_result_2_comb_jm_ns])\\\n","                            .drop(['pretrain_weights', 'embedding_structure'], axis=1).reset_index(drop=True)\n","df_result_2_set.to_excel(path_results+'nurse/One_leave_out/Respond_to_reviewers/LIWC_jmim_UMLS_jmim.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"03efcc70","metadata":{"id":"03efcc70"},"outputs":[],"source":["df_probs_last_4.to_excel(path_results+'Best_probability_for_each_approach/OASIS_clinical_(turn_taking_liwc_umls_transcription)_pt_(liwc_umls_transcription)_ns_last_date_time_probs.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"8bacdb19","metadata":{"id":"8bacdb19"},"outputs":[],"source":["df_probs_comb_4.to_excel(path_results+'Best_probability_for_each_approach/OASIS_clinical_(turn_taking_liwc_umls_transcription)_pt_(liwc_umls_transcription)_ns_combined_text_probs.xlsx', index=False)"]},{"cell_type":"markdown","id":"fcaf4c3e","metadata":{"id":"fcaf4c3e"},"source":["## Part 5:\n","## patient: (transformer on transcription + LIWC + Turn taking + Linguistic) +\n","## nurse: (transformer on transcription + LIWC)"]},{"cell_type":"code","execution_count":null,"id":"449c3274","metadata":{"id":"449c3274"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"6f7768bb","metadata":{"scrolled":true,"id":"6f7768bb"},"outputs":[],"source":["try:\n","    df_result_last_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/5_(turn_taking_liwc_linguistic)pt_(liwc)ns_(transcription)pt_(transcription)ns.xlsx')\n","    last_index = df_result_last_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_last_all.loc[last_index, 'Approach']\n","    second = df_result_last_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_last_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_last_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except:\n","    df_result_last_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_transcription = df_last_transcription.copy()\n","        df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_last_jm.copy()\n","        # nurse\n","        df_transcription_ns = df_last_transcription_ns.copy()\n","        df_fr_liwc_jm_ns = df_fr_last_liwc_jm_ns.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_transcription = df_comb_transcription.copy()\n","        df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","        df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","        df_frs_jm = df_without_umls_fr_comb_jm.copy()\n","        # nurse\n","        df_transcription_ns = df_comb_transcription_ns.copy()\n","        df_fr_liwc_jm_ns = df_fr_comb_liwc_jm_ns.copy()\n","\n","\n","    ##############\n","    df_result_bert_transc_jm, df_fr_bert_transc_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    df_result_bert_transc_jm_ns, df_fr_bert_transc_jm_ns = classifying_using_bert(df_transcription_ns, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True, nurse=True)\n","    ##############\n","    dataframes = [df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_liwc_jm_ns,\n","                  df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","    df_fr_6set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_1, df_fr_6set_jm_no_yes_jm = classifying_all(df_fr_6set_jm, clfs_list, clfs_name,\n","        approach=episode[0], pipline='linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt + '\\\n","        'liwc_jmim_ns + transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns'\n","        , pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    # 3_1_1_1 -> 1(3) + 1 + 1 + 1\n","    dataframes = [df_frs_jm, df_fr_liwc_jm_ns, df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","    df_fr_3_1_1_1set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_2, df_fr_3_1_1_1set_jm_no_yes_jm = classifying_all(df_fr_3_1_1_1set_jm, clfs_list, clfs_name,\n","        approach=episode[0], pipline='(linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt)jmim + '\\\n","        'liwc_jmim_ns + transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns'\n","        , pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    df_result_last_all = pd.concat([df_result_last_all, df_result_final_1, df_result_final_2]).reset_index(drop=True)\n","    df_result_last_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/5_(turn_taking_liwc_linguistic)pt_(liwc)ns_(transcription)pt_(transcription)ns.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"8694ab5e","metadata":{"id":"8694ab5e"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"30c2b148","metadata":{"id":"30c2b148"},"source":["### Best model probabilities of part 5"]},{"cell_type":"code","execution_count":null,"id":"6ebc0fe4","metadata":{"id":"6ebc0fe4","outputId":"a37e2257-a01f-4df6-a0c1-3bec880f623d"},"outputs":[{"data":{"text/plain":["('combined text', True, 'distilbert-base-uncased', 'first')"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["episodes[20]"]},{"cell_type":"code","execution_count":null,"id":"5aa28952","metadata":{"id":"5aa28952"},"outputs":[],"source":["episode = episodes[20]\n","\n","print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                  .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","if episode[0]=='last date time':\n","    df_transcription = df_last_transcription.copy()\n","    df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","    df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","    df_frs_jm = df_without_umls_fr_last_jm.copy()\n","    # nurse\n","    df_transcription_ns = df_last_transcription_ns.copy()\n","    df_fr_liwc_jm_ns = df_fr_last_liwc_jm_ns.copy()\n","\n","elif episode[0]=='combined text':\n","    df_transcription = df_comb_transcription.copy()\n","    df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","    df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","    df_frs_jm = df_without_umls_fr_comb_jm.copy()\n","    # nurse\n","    df_transcription_ns = df_comb_transcription_ns.copy()\n","    df_fr_liwc_jm_ns = df_fr_comb_liwc_jm_ns.copy()\n","\n","\n","##############\n","df_result_bert_transc_jm, df_fr_bert_transc_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True)\n","##############\n","df_result_bert_transc_jm_ns, df_fr_bert_transc_jm_ns = classifying_using_bert(df_transcription_ns, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True, nurse=True)\n","##############\n","dataframes = [df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_liwc_jm_ns,\n","              df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","df_fr_6set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","##############\n","df_probs_combined_5 = classifying_all(df_fr_6set_jm, [clfs_list[0]], [clfs_name[0]],\n","        get_probability=True, get_minmax=False,\n","        approach=episode[0], pipline='linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt + '\\\n","        'liwc_jmim_ns + transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n"]},{"cell_type":"code","execution_count":null,"id":"4d94260c","metadata":{"id":"4d94260c"},"outputs":[],"source":["df_probs_combined_5.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Best_probability_for_each_approach/5_(turn_taking_liwc_linguistic)pt_(liwc)ns_(transcription)pt_(transcription)ns_combined_text_probs.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"c170da1f","metadata":{"id":"c170da1f"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7a5a1a2b","metadata":{"id":"7a5a1a2b"},"source":["## Part 5_1:\n","## patient: (transformer on transcription) + nurse: (transformer on transcription)"]},{"cell_type":"code","execution_count":null,"id":"a5f5f032","metadata":{"id":"a5f5f032"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"ce603a5f","metadata":{"id":"ce603a5f"},"outputs":[],"source":["try:\n","    df_result_last_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/5_1_(transcription)pt_(transcription)ns.xlsx')\n","    last_index = df_result_last_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_last_all.loc[last_index, 'Approach']\n","    second = df_result_last_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_last_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_last_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except:\n","    df_result_last_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_transcription_pt = df_last_transcription.copy()\n","        # nurse\n","        df_transcription_ns = df_last_transcription_ns.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_transcription_pt = df_comb_transcription.copy()\n","        # nurse\n","        df_transcription_ns = df_comb_transcription_ns.copy()\n","\n","\n","    ##############\n","    df_result_bert_transc_jm_pt, df_fr_bert_transc_jm_pt = classifying_using_bert(df_transcription_pt, [clfs_list[3]], [clfs_name[3]],\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    df_result_bert_transc_jm_ns, df_fr_bert_transc_jm_ns = classifying_using_bert(df_transcription_ns, [clfs_list[3]], [clfs_name[3]],\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True, nurse=True)\n","    ##############\n","    df_fr_pt_ns_jm = pd.merge(df_fr_bert_transc_jm_pt, df_fr_bert_transc_jm_ns ,on=['study_id', 'outcome'], how='inner')\n","\n","    df_result_final, df_fr_pt_ns_jm_no_yes_jm = classifying_all(df_fr_pt_ns_jm, [clfs_list[3]], [clfs_name[3]],\n","        approach=episode[0], pipline='transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns'\n","        , pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","\n","    ##############\n","    df_result_last_all = pd.concat([df_result_last_all, df_result_final]).reset_index(drop=True)\n","    df_result_last_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/5_1_(transcription)pt_(transcription)ns.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"4bbcdd03","metadata":{"id":"4bbcdd03"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"cedd82ca","metadata":{"id":"cedd82ca"},"source":["## Part 6:\n","## patient: (OASIS + Clinical note) +\n","## patient: (transformer on transcription + LIWC + Turn taking + Linguistic) +\n","## nurse: (transformer on transcription + LIWC)"]},{"cell_type":"code","execution_count":null,"id":"a3db74a6","metadata":{"id":"a3db74a6"},"outputs":[],"source":["pretrain_weights_list = ['bert-base-uncased', 'distilbert-base-uncased', 'emilyalsentzer/Bio_ClinicalBERT']\n","embedding_structure_list = ['first', 'mean']\n","approach_list = ['last date time', 'combined text']\n","using_jmim_list = [False, True]\n","all_list = [approach_list, using_jmim_list, pretrain_weights_list, embedding_structure_list]\n","episodes = list(itertools.product(*all_list))"]},{"cell_type":"code","execution_count":null,"id":"f0fea206","metadata":{"scrolled":true,"id":"f0fea206"},"outputs":[],"source":["try:\n","    df_result_last_all = pd.read_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/6_OASIS_(linguistic_turn_taking_liwc)pt_(liwc)ns_clinical_(transcription)pt_(transcription)ns.xlsx')\n","    last_index = df_result_last_all.shape[0]-1\n","    last_run_tupple = ()\n","    first = df_result_last_all.loc[last_index, 'Approach']\n","    second = df_result_last_all.loc[last_index, 'Using_jmim']\n","    second = True if second=='Yes' else False\n","    third = df_result_last_all.loc[last_index, 'pretrain_weights']\n","    fourth = df_result_last_all.loc[last_index, 'embedding_structure']\n","    last_run_tupple = (first, second, third, fourth)\n","    run_again_idx = episodes.index(last_run_tupple)+1\n","except:\n","    df_result_last_all = pd.DataFrame()\n","    run_again_idx = 0\n","\n","for episode in episodes[run_again_idx:]:\n","\n","    print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                      .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","    if episode[0]=='last date time':\n","        df_clinical = df_last_clinical.copy()\n","        df_transcription = df_last_transcription.copy()\n","        df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","        df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","        df_frs_jm = df_fr_last_jm.copy()\n","        # nurse\n","        df_transcription_ns = df_last_transcription_ns.copy()\n","        df_fr_liwc_jm_ns = df_fr_last_liwc_jm_ns.copy()\n","        df_frs_jm_ns = df_fr_last_jm_ns.copy()\n","\n","    elif episode[0]=='combined text':\n","        df_clinical = df_comb_clinical.copy()\n","        df_transcription = df_comb_transcription.copy()\n","        df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","        df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","        df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","        df_frs_jm = df_fr_comb_jm.copy()\n","        # nurse\n","        df_transcription_ns = df_comb_transcription_ns.copy()\n","        df_fr_liwc_jm_ns = df_fr_comb_liwc_jm_ns.copy()\n","        df_frs_jm_ns = df_fr_comb_jm_ns.copy()\n","\n","    ##############\n","    df_result_bert_transc_jm, df_fr_bert_transc_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True)\n","    ##############\n","    df_result_bert_clinic_jm, df_fr_bert_clinic_jm = classifying_using_bert(df_clinical, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True, clinical=True)\n","    ##############\n","    df_result_bert_transc_jm_ns, df_fr_bert_transc_jm_ns = classifying_using_bert(df_transcription_ns, clfs_list, clfs_name,\n","                                                         approach=episode[0], pretrain_weights=episode[2],\n","                                                         embedding_structure=episode[3], using_jmim=True, nurse=True)\n","    ##############\n","    dataframes = [df_fr_os_jm, df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_liwc_jm_ns,\n","                  df_fr_bert_clinic_jm, df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","    df_fr_8set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_1, df_fr_8set_jm_no_yes_jm = classifying_all(df_fr_8set_jm, clfs_list, clfs_name,\n","        approach=episode[0], pipline='oasis_jmim + linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt + '\\\n","        'liwc_jmim_ns + transformer_on_clinical_jmim + '\\\n","        'transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    # 1_3_1_1_1_1 -> 1 + 1(3) + 1 + 1 + 1 + 1\n","    dataframes = [df_fr_os_jm, df_frs_jm, df_fr_liwc_jm_ns, df_fr_bert_clinic_jm, df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","    df_fr_1_3_1_1_1_1set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","    df_result_final_2, df_fr_1_3_1_1_1_1set_jm_no_yes_jm = classifying_all(df_fr_1_3_1_1_1_1set_jm, clfs_list, clfs_name,\n","        approach=episode[0], pipline='oasis_jmim + (linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt)jmim + '\\\n","        'liwc_jmim_ns + transformer_on_clinical_jmim + '\\\n","        'transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","    ##############\n","    df_result_last_all = pd.concat([df_result_last_all, df_result_final_1, df_result_final_2]).reset_index(drop=True)\n","    df_result_last_all.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/6_OASIS_(linguistic_turn_taking_liwc)pt_(liwc)ns_clinical_(transcription)pt_(transcription)ns.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"e90d26a7","metadata":{"id":"e90d26a7"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7ed92d26","metadata":{"id":"7ed92d26"},"source":["### Best model probabilities of part 6"]},{"cell_type":"code","execution_count":null,"id":"c6f2c6eb","metadata":{"id":"c6f2c6eb","outputId":"019420be-f188-4ae1-b494-70f25375a51e"},"outputs":[{"data":{"text/plain":["('combined text', False, 'bert-base-uncased', 'mean')"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["episodes[13]"]},{"cell_type":"code","execution_count":null,"id":"411ed96d","metadata":{"scrolled":true,"id":"411ed96d"},"outputs":[],"source":["episode = episodes[13]\n","\n","print('\\nApproach: {}\\t\\t  Using_jmim: {}\\nPretrain_weights: {}\\t  embedding_structure: {}'\n","                                                  .format(episode[0], episode[1], episode[2], episode[3]))\n","\n","if episode[0]=='last date time':\n","    df_clinical = df_last_clinical.copy()\n","    df_transcription = df_last_transcription.copy()\n","    df_fr_ling_jm = df_fr_last_ling_jm.copy()\n","    df_fr_liwc_jm = df_fr_last_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_last_turn_jm.copy()\n","    df_fr_umls_jm = df_fr_last_umls_jm.copy()\n","    df_frs_jm = df_fr_last_jm.copy()\n","    # nurse\n","    df_transcription_ns = df_last_transcription_ns.copy()\n","    df_fr_liwc_jm_ns = df_fr_last_liwc_jm_ns.copy()\n","    df_fr_umls_jm_ns = df_fr_last_umls_jm_ns.copy()\n","    df_frs_jm_ns = df_fr_last_jm_ns.copy()\n","\n","elif episode[0]=='combined text':\n","    df_clinical = df_comb_clinical.copy()\n","    df_transcription = df_comb_transcription.copy()\n","    df_fr_ling_jm = df_fr_comb_ling_jm.copy()\n","    df_fr_liwc_jm = df_fr_comb_liwc_jm.copy()\n","    df_fr_turn_jm = df_fr_mean_turn_jm.copy()\n","    df_fr_umls_jm = df_fr_comb_umls_jm.copy()\n","    df_frs_jm = df_fr_comb_jm.copy()\n","    # nurse\n","    df_transcription_ns = df_comb_transcription_ns.copy()\n","    df_fr_liwc_jm_ns = df_fr_comb_liwc_jm_ns.copy()\n","    df_fr_umls_jm_ns = df_fr_comb_umls_jm_ns.copy()\n","    df_frs_jm_ns = df_fr_comb_jm_ns.copy()\n","\n","##############\n","df_result_bert_transc_jm, df_fr_bert_transc_jm = classifying_using_bert(df_transcription, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True)\n","##############\n","df_result_bert_clinic_jm, df_fr_bert_clinic_jm = classifying_using_bert(df_clinical, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True, clinical=True)\n","##############\n","df_result_bert_transc_jm_ns, df_fr_bert_transc_jm_ns = classifying_using_bert(df_transcription_ns, clfs_list, clfs_name,\n","                                                     approach=episode[0], pretrain_weights=episode[2],\n","                                                     embedding_structure=episode[3], using_jmim=True, nurse=True)\n","##############\n","dataframes = [df_fr_os_jm, df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_liwc_jm_ns,\n","              df_fr_bert_clinic_jm, df_fr_bert_transc_jm, df_fr_bert_transc_jm_ns]\n","df_fr_8set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","##############\n","df_probs_combined_6 = classifying_all(df_fr_8set_jm, [clfs_list[1]], [clfs_name[1]],\n","        get_probability=True, get_minmax=False,\n","        approach=episode[0], pipline='oasis_jmim + linguistic_jmim_pt + turn_taking_jmim_pt + liwc_jmim_pt + '\\\n","        'liwc_jmim_ns + transformer_on_clinical_jmim + '\\\n","        'transformer_on_transcription_jmim_pt + transformer_on_transcription_jmim_ns',\n","        pretrain_weights=episode[2], embedding_structure=episode[3], using_jmim=episode[1])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"be2bbeb6","metadata":{"id":"be2bbeb6"},"outputs":[],"source":["df_probs_combined_6.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Best_probability_for_each_approach/6_OASIS_(linguistic_turn_taking_liwc)pt_(liwc)ns_clinical_(transcription)pt_(transcription)ns_combined_text_probs.xlsx', index=False)"]},{"cell_type":"markdown","id":"1256fa79","metadata":{"id":"1256fa79"},"source":["#### Most important features of part 6"]},{"cell_type":"code","execution_count":null,"id":"137b56bc","metadata":{"id":"137b56bc"},"outputs":[],"source":["# These are not important for jmim features ([clfs_list[1]], [clfs_name[1]])\n","df_fr_8set_jm_jmim = classifying_all(df_fr_8set_jm, [clfs_list[1]], [clfs_name[1]], using_jmim=True, most_important_features=True)\n","df_fr_8set_jm_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/OASIS_(linguistic_turn_taking_liwc)pt_(liwc)ns_clinical_(transcription)pt_(transcription)ns_combined_text.xlsx', index=False)"]},{"cell_type":"markdown","id":"7384c3b0","metadata":{"id":"7384c3b0"},"source":["#### Most important features of part 6 without any transcription and clinical note"]},{"cell_type":"code","execution_count":null,"id":"d398e37e","metadata":{"id":"d398e37e"},"outputs":[],"source":["dataframes = [df_fr_os_jm, df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_liwc_jm_ns]\n","df_fr_5set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","# These are not important for jmim features ([clfs_list[1]], [clfs_name[1]])\n","df_fr_5set_jm_jmim = classifying_all(df_fr_5set_jm, [clfs_list[1]], [clfs_name[1]], using_jmim=True, most_important_features=True)\n","df_fr_5set_jm_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/OASIS_(linguistic_turn_taking_liwc)pt_(liwc)ns.xlsx', index=False)"]},{"cell_type":"markdown","id":"5d973dae","metadata":{"id":"5d973dae"},"source":["#### Most important features of part 6 with UMLS and without any transcription and clinical note"]},{"cell_type":"code","execution_count":null,"id":"0e4f43b1","metadata":{"id":"0e4f43b1"},"outputs":[],"source":["dataframes = [df_fr_os_jm, df_fr_ling_jm, df_fr_turn_jm, df_fr_liwc_jm, df_fr_umls_jm, df_fr_liwc_jm_ns, df_fr_umls_jm_ns]\n","df_fr_7set_jm = reduce(lambda left,right: pd.merge(left, right ,on=['study_id', 'outcome'], how='inner'), dataframes)\n","\n","# These are not important for jmim features ([clfs_list[1]], [clfs_name[1]])\n","df_fr_7set_jm_jmim = classifying_all(df_fr_7set_jm, [clfs_list[1]], [clfs_name[1]], using_jmim=True, most_important_features=True, n_features_jmim=40)\n","df_fr_7set_jm_jmim.to_excel(path_results+'patient/One_leave_out/Respond_to_reviewers/Most_important_features/OASIS_(linguistic_turn_taking_liwc_umls)pt_(liwc_umls)ns.xlsx', index=False)"]},{"cell_type":"code","execution_count":null,"id":"7570a3ca","metadata":{"id":"7570a3ca"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}